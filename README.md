# Incorporating Advances in Large Language Models (LLMs)

Resource page for advances in large language models

# Understanding Fundamental Research

In order to understand concepts from the current generation of large language model research check out:

Concepts around Transformers
1. [Attention Is All You Need research paper by Vaswani, Ashish, et al.](https://arxiv.org/abs/1706.03762)
2. [The Illustrated Transformer blog by Jay Alammar](https://jalammar.github.io/illustrated-transformer/)

Intuitions around why Supervised Fine Tuning (SFT) & Reinforcement Learning with Human Feedback (RLHF) work
1. [RLHF: Reinforcement Learning from Human Feedback blog by Chip Huyen](https://huyenchip.com/2023/05/02/rlhf.html)

# Current Generation of Models

1. [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)
2. [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
3. [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b)
4. [MPT-7B-8K: 8K Context Length for Document Understanding](https://www.mosaicml.com/blog/long-context-mpt-7b-8k)
